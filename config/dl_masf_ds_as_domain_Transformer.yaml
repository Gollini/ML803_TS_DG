name: "dl_masf_ds_as_domain_Transformer"

feature_definition:
  use_norm_features: True

data_loader:
  batch_size: 128
  # Define how each data batch is sampled from multiple datasets and/or people
  # option: "across_dataset", "within_dataset", "across_person", "within_person"
  generate_by: "within_dataset" 
  
  mixup: null # option to mixup data: null, "across" (person or dataset), "within" (person or dataset)
  mixup_alpha: null # the strongness of mixup. from 0 to 1

model_params:
  arch: "Transformer" # model architecture
  # input dimension
  # 2: a multi-channel time series data (for models like 1dCNN)
  # 3: a single-channel image data (for models like 2dCNN)
  input_dim: 2
  head_size: 4 # size of each head in the MultiHeadAttention layer
  num_heads: 4 # number of head in the MultiHeadAttention layer
  ff_dim: 16 # hidden layer size of the fully connected forward layer
  num_transformer_blocks: 2 # number of transformer blocks
  transfromer_dropout: 0.25 # dropout rate within a transformer block
  mlp_units: [32] # size of fully connected layers after the transformer blocks
  mlp_dropout: 0.25 # dropout rate of between filly connected layers
  embedding_size: 8 # vector length of the feature embedding network output
  flag_y_vector: True # True: one hot vector on y for regular training
  metric_network_fc_shapes: [16, 8] # size of fully connected layers for the metric network
  kl_divergence_temperature: 2.0 # temperature parameter or KL divergence computing

  meta_training_proportion: 0.99 # portion of data to be used as the meta-train set, (1-portion) for the meta-test set
  triplet_loss_margin: 0.2 # triplet loss margin
  global_loss_weight: 1.0 # the weight of global loss item in the loss function
  local_loss_weight: 0.5 # the weight of local loss item in the loss function


training_params:
  inner_optimizer: "Adam" # option: SGD or Adam
  inner_learning_rate: 0.0005
  outer_optimizer: "Adam" # option: SGD or Adam
  outer_learning_rate: 0.0005
  metric_network_optimizer: "Adam" # option: SGD or Adam
  metric_network_learning_rate: 0.0005
  gradient_clip_norm: 2.0

  epochs: 200
  steps_per_epoch: 2 # number of batches per epoch
  cos_annealing_step: 20
  cos_annealing_decay: 0.95
  best_epoch_strategy: "direct" # option: direct or on_test
  verbose: 0
  skip_training: False
